{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65385575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4f8227",
   "metadata": {},
   "source": [
    "# Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e000d21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_UE = 5 # number of terminals\n",
    "num_AP = 30 # number of access points\n",
    "B = 20 # Mhz\n",
    "D = 1 # km\n",
    "tau=10\n",
    "random_matrix = np.random.randn(tau, tau)\n",
    "U, S, V = np.linalg.svd(random_matrix)\n",
    "\n",
    "Hb = 15 # Base station height in m\n",
    "Hm = 1.65 # Mobile height in m\n",
    "f = 1900 # Frequency in MHz\n",
    "aL = (1.1 * np.log10(f) - 0.7) * Hm - (1.56 * np.log10(f) - 0.8)\n",
    "L = 46.3+33.9*np.log10(f)-13.82*np.log10(Hb)-aL\n",
    "\n",
    "power_f=0.2 # downlink power\n",
    "rho_p, rho_d = power_f, power_f\n",
    "\n",
    "# Pd = power_f / 10 ** ((-203.975 + 10 * np.log10(20 * 10 ** 6) + 9) / 10) # normalized receive SNR\n",
    "Ther_noise = 20000000 * 10**(-17.4) * 10**-3\n",
    "Pd = 1/Ther_noise\n",
    "Pu=Pd\n",
    "\n",
    "d0=0.01 # km\n",
    "d1=0.05 # km\n",
    "\n",
    "num_antenna = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4b13b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 500\n",
    "num_test = 200\n",
    "batchSize = 32\n",
    "\n",
    "lr = 1e-4\n",
    "step_size = 5\n",
    "gamma = 0.9\n",
    "\n",
    "# FL\n",
    "num_rounds = 5\n",
    "\n",
    "num_client = num_AP \n",
    "num_epochs = 10\n",
    "eval_round = num_rounds//10 if num_rounds//10 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386217ba",
   "metadata": {},
   "source": [
    "# Create data loader for training and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "013dfd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.data_gen import Generate_Input, create_graph\n",
    "\n",
    "Beta_all, Phi_all = Generate_Input(num_train, tau, num_UE, num_AP, Pd, D=1, Hb=15, Hm=1.65, f=1900,\n",
    "                    var_noise=1, Pmin=0, seed=2017, d0=d0, d1=d1)\n",
    "train_data = create_graph(Beta_all, Phi_all, 'het')\n",
    "train_loader = [\n",
    "    DataLoader(train_data[i], batch_size=batchSize, shuffle=True)\n",
    "    for i in range(num_AP)\n",
    "]\n",
    "\n",
    "\n",
    "Beta_test, Phi_test = Generate_Input(num_test, tau, num_UE, num_AP, Pd, D=1, Hb=15, Hm=1.65, f=1900,\n",
    "                    var_noise=1, Pmin=0, seed=2017, d0=d0, d1=d1)\n",
    "test_data = create_graph(Beta_test, Phi_test, 'het')\n",
    "test_loader = [\n",
    "    DataLoader(test_data[i], batch_size=batchSize, shuffle=False)\n",
    "    for i in range(num_AP)\n",
    "]\n",
    "\n",
    "\n",
    "train_data_cen = create_graph(Beta_all, Phi_all, 'het', isDecentralized=False)\n",
    "train_loader_cen = DataLoader(train_data_cen, batch_size=batchSize, shuffle=True)\n",
    "test_data_cen = create_graph(Beta_test, Phi_test, 'het', isDecentralized=False)\n",
    "test_loader_cen = DataLoader(test_data_cen, batch_size=batchSize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dbcf6e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_channels = 32 # > 4\n",
    "num_gnn_layers = 2\n",
    "\n",
    "\n",
    "ap_dim = train_data[0][0]['AP'].x.shape[1]\n",
    "ue_dim = train_data[0][0]['UE'].x.shape[1]\n",
    "edge_dim = train_data[0][0]['down'].edge_attr.shape[1]\n",
    "tt_meta = [('UE', 'up', 'AP'), ('AP', 'down', 'UE')]\n",
    "dim_dict = {\n",
    "    'UE': ue_dim,\n",
    "    'AP': ap_dim,\n",
    "    'edge': edge_dim,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "99d459b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.GNN import APHetNet\n",
    "\n",
    "model = APHetNet(\n",
    "    metadata=tt_meta,\n",
    "    dim_dict=dim_dict,\n",
    "    out_channels=hidden_channels,\n",
    "    num_layers=num_gnn_layers,\n",
    "    hid_layers=hidden_channels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d35ce82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.GNN import APHetNet\n",
    "from Utils.training import train, eval, package_calculate\n",
    "from Utils.synthetic_graph import return_graph, combine_graph\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "global_model = APHetNet(\n",
    "    metadata=tt_meta,\n",
    "    dim_dict=dim_dict,\n",
    "    out_channels=hidden_channels,\n",
    "    num_layers=num_gnn_layers,\n",
    "    hid_layers=hidden_channels\n",
    ").to(device)\n",
    "local_models, optimizers = [], []\n",
    "\n",
    "# Init every client model/optimizer\n",
    "for each_AP in range(num_AP):\n",
    "    model = APHetNet(\n",
    "        metadata=tt_meta,\n",
    "        dim_dict=dim_dict,\n",
    "        out_channels=hidden_channels,\n",
    "        num_layers=num_gnn_layers,\n",
    "        hid_layers=hidden_channels\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    local_models.append(model)\n",
    "    optimizers.append(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a70bdca",
   "metadata": {},
   "source": [
    "# Main Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d36441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5e44fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Federated Learning with 30 clients for 5 rounds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[152], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad() \n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 25\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mfl_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponses_ap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpower_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho_d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpower_f\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_antenna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_antenna\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m local_weights\u001b[38;5;241m.\u001b[39mappend(copy\u001b[38;5;241m.\u001b[39mdeepcopy(model\u001b[38;5;241m.\u001b[39mstate_dict()))\n\u001b[1;32m     30\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\n",
      "File \u001b[0;32m~/Documents/Giang/GNN_FL_cf_mMIMO/Utils/training.py:344\u001b[0m, in \u001b[0;36mfl_train\u001b[0;34m(dataLoader, responseInfo, model, optimizer, tau, rho_p, rho_d, num_antenna)\u001b[0m\n\u001b[1;32m    342\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    343\u001b[0m num_graph \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mnum_graphs\n\u001b[0;32m--> 344\u001b[0m x_dict, edge_dict, edge_index \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(\n\u001b[1;32m    346\u001b[0m     batch, x_dict, response, \n\u001b[1;32m    347\u001b[0m     tau\u001b[38;5;241m=\u001b[39mtau, rho_p\u001b[38;5;241m=\u001b[39mrho_p, rho_d\u001b[38;5;241m=\u001b[39mrho_d, num_antenna\u001b[38;5;241m=\u001b[39mnum_antenna\n\u001b[1;32m    348\u001b[0m )\n\u001b[1;32m    349\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/GNN_FL/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Giang/GNN_FL_cf_mMIMO/Models/GNN.py:226\u001b[0m, in \u001b[0;36mAPHetNet.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    224\u001b[0m x_dict, edge_index_dict, edge_attr_dict \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mx_dict, batch\u001b[38;5;241m.\u001b[39medge_index_dict, batch\u001b[38;5;241m.\u001b[39medge_attr_dict\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs:\n\u001b[0;32m--> 226\u001b[0m     x_dict, edge_attr_dict \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m dl_power \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpower(x_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUE\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    229\u001b[0m x_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUE\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUE\u001b[39m\u001b[38;5;124m'\u001b[39m][:,:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mue_dim], dl_power], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/GNN_FL/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Giang/GNN_FL_cf_mMIMO/Models/GNN.py:135\u001b[0m, in \u001b[0;36mAPConvLayer.forward\u001b[0;34m(self, x_dict, edge_index_dict, edge_attr_dict)\u001b[0m\n\u001b[1;32m    133\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39m(x_src, x_dst), edge_attr\u001b[38;5;241m=\u001b[39medge_attr_dict[edge_type], edge_type\u001b[38;5;241m=\u001b[39medge_type)\n\u001b[1;32m    134\u001b[0m tmp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x_dst, out], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 135\u001b[0m tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupd\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdst_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m src_init_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_init_dict[dst_type]\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_dim_dict[dst_type] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channel:\n",
      "File \u001b[0;32m~/miniconda3/envs/GNN_FL/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/GNN_FL/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/GNN_FL/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/GNN_FL/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:150\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    146\u001b[0m     exponential_average_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_batches_tracked\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_batches_tracked\u001b[38;5;241m.\u001b[39madd_(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# use cumulative moving average\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/GNN_FL/lib/python3.10/site-packages/torch/nn/modules/module.py:1601\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_backward_pre_hooks\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1599\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m-> 1601\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   1602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1603\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from Utils.training import (\n",
    "    get_global_info, distribute_global_info, average_weights,\n",
    "    fl_train, fl_eval, fl_eval_rate\n",
    ")\n",
    "\n",
    "log = []\n",
    "\n",
    "print(f\"Starting Federated Learning with {num_client} clients for {num_rounds} rounds\")\n",
    "\n",
    "for round in range(num_rounds):\n",
    "    # print(f\"\\n=== Federated Round {round+1}/{num_rounds} ===\")\n",
    "    \n",
    "    ## 1.Exchange global information\n",
    "    send_to_server = get_global_info(\n",
    "        train_loader, local_models, optimizers,\n",
    "        tau=tau, rho_p=power_f, rho_d=power_f\n",
    "    )\n",
    "    response_all = distribute_global_info(send_to_server)\n",
    "    \n",
    "    \n",
    "    ## 2. Training Local models    \n",
    "    local_weights = []\n",
    "    total_loss = 0.0\n",
    "    for model, opt, batches , responses_ap in zip(local_models, optimizers, train_loader, response_all):\n",
    "        model.train() \n",
    "        opt.zero_grad() \n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss = fl_train(\n",
    "                batches, responses_ap, model, opt,\n",
    "                tau=tau, rho_p=power_f, rho_d=power_f, num_antenna=num_antenna\n",
    "            )\n",
    "        local_weights.append(copy.deepcopy(model.state_dict()))\n",
    "        total_loss += train_loss\n",
    "    avg_loss = total_loss / num_client\n",
    "    # print(f\"Round {round+1}: Average local training loss = {avg_loss:.6f}\")\n",
    "\n",
    "\n",
    "    ## 3. Update global models\n",
    "    global_weights = average_weights(local_weights)\n",
    "\n",
    "    # Broadcast updated global weights to all clients\n",
    "    for model in local_models:\n",
    "        model.load_state_dict(global_weights)\n",
    "        \n",
    "\n",
    "    ## 4. Exchange global eval information\n",
    "    # print(\"Evaluating global model(s)...\")\n",
    "    # send_to_server_eval = get_global_info(\n",
    "    #     test_loader, local_models, optimizers,\n",
    "    #     tau=tau, rho_p=power_f, rho_d=power_f\n",
    "    # )\n",
    "    # response_all_eval = distribute_global_info(send_to_server_eval)\n",
    "    # total_eval_rate = 0.0\n",
    "    # for client_idx, (model, loader, responses) in enumerate(zip(local_models, test_loader, response_all_eval)):\n",
    "    #     model.eval() \n",
    "    #     for epoch in range(num_epochs):\n",
    "    #         eval_metrics = fl_eval(\n",
    "    #             loader, responses, model,\n",
    "    #             tau=tau, rho_p=power_f, rho_d=power_f, num_antenna=num_antenna\n",
    "    #         )\n",
    "    #     total_eval_rate += eval_metrics\n",
    "    # total_eval_rate = total_eval_rate/num_client\n",
    "    #     # print(f\"Client {client_idx}: {eval_metrics}\")\n",
    "    \n",
    "    \n",
    "    total_eval_rate = fl_eval_rate(\n",
    "        test_loader, model,\n",
    "        tau=tau, rho_p=power_f, rho_d=power_f, num_antenna=num_antenna\n",
    "    )\n",
    "    if round%eval_round==0:\n",
    "        print(f\"Round {round+1:02d}/{num_rounds}: Avg Training Rate = {-avg_loss:.6f} | Avg Eval rate = {total_eval_rate:.6f}\")\n",
    "            \n",
    "    log.append({\n",
    "        \"round\": round + 1,\n",
    "        \"train_loss\": avg_loss,\n",
    "        \"eval\": total_eval_rate\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d1d6522a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "753941a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "all_power = []\n",
    "all_large_scale = []\n",
    "\n",
    "all_phi = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batches_at_k in enumerate(zip(*test_loader)):\n",
    "        per_batch_power = []\n",
    "        per_batch_large_scale = []\n",
    "        for ap_idx, (model, batch) in enumerate(zip(local_models, batches_at_k)):\n",
    "            model.eval()\n",
    "            # iterate over all batch of each AP\n",
    "            batch = batch.to(device)\n",
    "            num_graphs = batch.num_graphs\n",
    "            num_UEs = batch['UE'].x.shape[0]//num_graphs\n",
    "            num_APs = batch['AP'].x.shape[0]//num_graphs\n",
    "            \n",
    "            x_dict, edge_dict, edge_index = model(batch)\n",
    "            power = x_dict['UE'].reshape(num_graphs, num_UEs, -1)\n",
    "            power_matrix = power[:,:,-1][:, None, :]\n",
    "            pilot_matrix = batch['UE'].x.reshape(num_graphs, num_UEs, -1)\n",
    "            large_scale = batch['AP','down','UE'].edge_attr.reshape(num_graphs, num_APs, num_UEs)\n",
    "            \n",
    "            per_batch_power.append(power_matrix)\n",
    "            per_batch_large_scale.append(large_scale)\n",
    "            # per_batch_phi.append(power_matrix)\n",
    "        per_batch_phi = pilot_matrix\n",
    "        per_batch_power = torch.cat(per_batch_power, dim=1)\n",
    "        per_batch_large_scale = torch.cat(per_batch_large_scale, dim=1)\n",
    "         \n",
    "        all_power.append(per_batch_power)\n",
    "        all_large_scale.append(per_batch_large_scale)\n",
    "        all_phi.append(per_batch_phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "6fc980c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.655353698730469"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Utils.training import rate_calculation, variance_calculate\n",
    "total_min_rate = 0.0\n",
    "total_samples = 0.0\n",
    "for each_power, each_large_scale, each_phi in zip(all_power, all_large_scale, all_phi):\n",
    "    num_graphs = len(each_power)\n",
    "    each_channel_variance = variance_calculate(each_large_scale, each_phi, tau=tau, rho_p=rho_p)\n",
    "    rate = rate_calculation(each_power, each_large_scale, each_channel_variance, each_phi, rho_d=rho_d, num_antenna=num_antenna)\n",
    "    min_rate, _ = torch.min(rate, dim=1)\n",
    "    min_rate = torch.mean(min_rate)\n",
    "    total_min_rate += min_rate.item() * num_graphs\n",
    "    total_samples += num_graphs\n",
    "total_min_rate/total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f29e310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete!\n",
      "Final Round | Avg Train Loss: -5.516941 | Avg Eval: 5.171294670104981\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Final Round | Avg Train Loss: {log[-1]['train_loss']:.6f} | Avg Eval: {log[-1]['eval']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19086df",
   "metadata": {},
   "source": [
    "# Centralized training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3b6b6b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.GNN import APHetNet\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
    "\n",
    "cen_model = APHetNet(\n",
    "    metadata=tt_meta,\n",
    "    dim_dict=dim_dict,\n",
    "    out_channels=hidden_channels,\n",
    "    num_layers=num_gnn_layers,\n",
    "    hid_layers=hidden_channels,\n",
    "    edge_conv=True\n",
    ").to(device)\n",
    "cen_optimizer = torch.optim.Adam(cen_model.parameters(), lr=lr)\n",
    "cen_scheduler = StepLR(cen_optimizer, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c0e65388",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "eval_epochs = num_epochs//10 if num_epochs//10 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4a81381f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/500 | Train Loss: -0.974967 | Train Rate: 0.860557 | Test Rate: 0.815201 \n",
      "Epoch 051/500 | Train Loss: -1.692961 | Train Rate: 1.788714 | Test Rate: 1.751839 \n",
      "Epoch 101/500 | Train Loss: -1.897718 | Train Rate: 1.816060 | Test Rate: 1.841762 \n",
      "Epoch 151/500 | Train Loss: -1.918362 | Train Rate: 1.705942 | Test Rate: 1.845506 \n",
      "Epoch 201/500 | Train Loss: -1.926587 | Train Rate: 1.762285 | Test Rate: 1.931217 \n",
      "Epoch 251/500 | Train Loss: -1.783407 | Train Rate: 1.703272 | Test Rate: 1.828142 \n",
      "Epoch 301/500 | Train Loss: -1.983649 | Train Rate: 1.928573 | Test Rate: 1.871372 \n",
      "Epoch 351/500 | Train Loss: -1.791572 | Train Rate: 1.922842 | Test Rate: 1.812562 \n",
      "Epoch 401/500 | Train Loss: -1.881939 | Train Rate: 1.722239 | Test Rate: 1.850021 \n",
      "Epoch 451/500 | Train Loss: -1.861650 | Train Rate: 1.798852 | Test Rate: 1.782266 \n"
     ]
    }
   ],
   "source": [
    "from Utils.training import cen_eval, cen_train\n",
    "for epoch in range(num_epochs):\n",
    "    cen_model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_eval = cen_eval(\n",
    "            train_loader_cen, cen_model,\n",
    "            tau=tau, rho_p=power_f, rho_d=power_f, num_antenna=num_antenna\n",
    "        )\n",
    "        test_eval = cen_eval(\n",
    "            train_loader_cen, cen_model,\n",
    "            tau=tau, rho_p=power_f, rho_d=power_f, num_antenna=num_antenna\n",
    "        )\n",
    "    cen_model.train()\n",
    "    train_loss = cen_train(\n",
    "        train_loader_cen, cen_model, cen_optimizer,\n",
    "        tau=tau, rho_p=power_f, rho_d=power_f, num_antenna=num_antenna\n",
    "    )\n",
    "    cen_scheduler.step()\n",
    "    if epoch%eval_epochs==0:\n",
    "        print(\n",
    "            f\"Epoch {epoch+1:03d}/{num_epochs} | \"\n",
    "            f\"Train Loss: {train_loss:.6f} | \"\n",
    "            f\"Train Rate: {train_eval:.6f} | \"\n",
    "            f\"Test Rate: {test_eval:.6f} \"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b989e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN_FL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
